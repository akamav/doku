In the neural network terminology:
one epoch = one forward pass and one backward pass of all the training examples
batch size = the number of training examples in one forward/backward pass. The higher the batch size, the more memory space you'll need.
number of iterations = number of passes, each pass using [batch size] number of examples. To be clear, one pass = one forward pass + one backward pass (we do not count the forward pass and backward pass as two different passes).
Example: if you have 1000 training examples, and your batch size is 500, then it will take 2 iterations to complete 1 epoch.


https://stats.stackexchange.com/questions/164876/tradeoff-batch-size-vs-number-of-iterations-to-train-a-neural-network

./freeze_graph --input_graph=./graph.pbtxt --input_checkpoint=./model.ckpt-100 --output_graph=frozen_graph.pb --output_node_names=CifarNet/Predictions/Softmax

tensorboard --logdir=/home/ladmin/akamav_ws/src/image_classifier/Testing/models/slim/tmp/train

http://warmspringwinds.github.io/tensorflow/tf-slim/2016/10/30/image-classification-and-segmentation-using-tensorflow-and-tf-slim/

https://www.kernix.com/blog/image-classification-with-a-pre-trained-deep-neural-network_p11

https://medium.com/@hamedmp/exporting-trained-tensorflow-models-to-c-the-right-way-cf24b609d183


